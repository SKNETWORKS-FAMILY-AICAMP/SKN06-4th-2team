import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from cachetools import LRUCache
from time import sleep
from dotenv import load_dotenv
from textwrap import dedent
import traceback

load_dotenv()


class Chatting:
    """
    ëŒ€í™”í˜• AI ì±„íŒ… í´ë˜ìŠ¤.
    GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì™€ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•˜ê³ , ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    def __init__(self, vector_store, cache_size=100, rate_limit=0.1):
        self.rate_limit = rate_limit  # API í˜¸ì¶œ ì†ë„ ì œí•œ (ì´ˆ)
        self.cache = LRUCache(maxsize=cache_size)  # ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
        # print(vector_store._collection.count() ) # vectoryê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        # ëª¨ë¸ê³¼ ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        self.model = ChatOpenAI(model="gpt-4o-mini")
        self.retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 20,
                "fetch_k": 200,
                "lambda_mult": 0.7,
            }
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì •ì˜)
        self.prompt_template = dedent("""
    ë‹¹ì‹ ì€ í•œêµ­ì˜ ì‹ë‹¹ì„ ì¶”ì²œí•˜ê³  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë¹„ì„œì…ë‹ˆë‹¤.  
    ì•„ë˜ [context]ì™€ [history]ì— ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.  
    [context]ë‚˜ [history]ì— ì—†ëŠ” ì •ë³´ì— ëŒ€í•´ì„œëŠ” ë°˜ë“œì‹œ **"ë¬¸ë§¥ì— ì—†ìŠµë‹ˆë‹¤"**ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.  

    ---

    ### **[ëŒ€í™” ê·œì¹™]**
    1. **ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©**  
        - [context]ë‚˜ [history]ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ìœ ì¶”í•˜ì§€ ë§ˆì„¸ìš”.
        - ì—†ëŠ” ì •ë³´ì— ëŒ€í•´ì„œëŠ” **"ë¬¸ë§¥ì— ì—†ìŠµë‹ˆë‹¤"**ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    2. **ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë‹µë³€**  
        - **ê°€ê²Œ ì´ë¦„**ì€ `**êµµê²Œ ê°•ì¡°**`í•˜ì„¸ìš”.  
        - ì„¸ë¶€ ì •ë³´ëŠ” `- í•­ëª©ë³„ ë¦¬ìŠ¤íŠ¸` í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.  
        - ì „í™”ë²ˆí˜¸, ê°€ê²©ëŒ€, ìœ„ì¹˜ ë“±ì˜ ì •ë³´ëŠ” `**ê°•ì¡°**`í•˜ì—¬ êµ¬ë¶„í•˜ì„¸ìš”.  
        - **ë¦¬ë³¸ê°œìˆ˜(í‰ì )ëŠ” í•­ìƒ í¬í•¨**í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.  

    3. **ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë‹µë³€ ì§€ì¹¨**
        - **ë‹¨ì¼ ê²€ìƒ‰ ìš”ì²­**: í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬, ê°€ê²©, ë¦¬ë³¸ê°œìˆ˜ ë“± íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ë‹¨ì¼ ì‹ë‹¹ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.  
            - ì˜ˆì‹œ 1: "ë¦¬ë³¸ê°œìˆ˜ê°€ 2ê°œì¸ ì‹ë‹¹ì€?"  
            - ì˜ˆì‹œ 2: "ë¦¬ë³¸ê°œìˆ˜ê°€ ë‘ ê°œì¸ ì‹ë‹¹ì€?"  
            - ë‹µë³€: "**ë¦¬ë³¸ê°œìˆ˜ê°€ 2ê°œì¸ ì‹ë‹¹ì€** ì‹ë‹¹ Aì…ë‹ˆë‹¤."
        - **ë¹„êµ ìš”ì²­**: 2ê°œ ì´ìƒì˜ ì‹ë‹¹ ì •ë³´ë¥¼ ë¹„êµí•˜ì—¬ ëª…í™•íˆ ì œê³µí•˜ì„¸ìš”.  
            - ì˜ˆì‹œ: "ë¦¬ë³¸ê°œìˆ˜ê°€ 2ê°œì¸ ì‹ë‹¹ê³¼ 3ê°œì¸ ì‹ë‹¹ì„ ë¹„êµí•´ì£¼ì„¸ìš”."  
            - ë‹µë³€ í˜•ì‹:  
                - **ì‹ë‹¹ A**: ë¦¬ë³¸ê°œìˆ˜ 2, ê°€ê²© **2ë§Œì›**, ì¹´í…Œê³ ë¦¬ **í•œì‹**  
                - **ì‹ë‹¹ B**: ë¦¬ë³¸ê°œìˆ˜ 3, ê°€ê²© **3ë§Œì›**, ì¹´í…Œê³ ë¦¬ **í”„ë‘ìŠ¤ ìš”ë¦¬**  

    4. **ì´ì „ ëŒ€í™” ì—°ì†ì„± ìœ ì§€**
        - ì§ˆë¬¸ì— **"ë‘ ê³³ ì¤‘", "ì´ ì¤‘ì—ì„œ", "ê°€ì¥ ì¢‹ì€ ê³³", "ë‘˜ ë‹¤"** ë“±ì˜ í‘œí˜„ì´ ë“±ì¥í•  ê²½ìš°, [history]ì—ì„œ **ê°€ì¥ ìµœê·¼ì— ë“±ì¥í•œ ë‘ ê°œì˜ ê°€ê²Œ ì´ë¦„**ì„ ì°¾ì•„ì„œ [context]ì—ì„œ í•´ë‹¹ ê°€ê²Œ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.
        - "ë¦¬ë³¸ê°œìˆ˜"ë¥¼ ëª…í™•íˆ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ìˆ«ì(1, 2, 3)ë¿ë§Œ ì•„ë‹ˆë¼ í•œê¸€ í‘œí˜„(í•œ ê°œ, ë‘ ê°œ, ì„¸ ê°œ)ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ì„¸ìš”.

    ---

    ### **[ì´ì „ ëŒ€í™”]**
    {history}

    ### **[í˜„ì¬ ì°¸ê³  ê°€ëŠ¥í•œ ì •ë³´]**
    {context}

    ---

    ### **ì§ˆë¬¸**: {question}  
    ### **ë‹µë³€**:
""")


    def get_cached_relevant_documents(self, query):
        """
        ê²€ìƒ‰ ìš”ì²­ì— ëŒ€í•´ ìºì‹± ë° API í˜¸ì¶œ ì†ë„ ì œí•œ ì ìš©.
        """
        if query in self.cache:
            return self.cache[query]
        try:
            sleep(self.rate_limit)  # API í˜¸ì¶œ ê°„ ë”œë ˆì´
            docs = self.retriever.invoke(query)
            context = "\n\n".join([doc.page_content for doc in docs])
            self.cache[query] = context
            return context
        except Exception as e:
            print(f"Error during document retrieval: {e}")
            traceback.print_exc()
            return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def send_message(self, message, history=None):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        try:
            context = self.get_cached_relevant_documents(message)

            history_text = ""
            for h in history:
                if h[0] == "human":
                    history_text += f"\nì‚¬ìš©ì: {h[1]}"
                else:
                    history_text += f"\nAI: {h[1]}"

            prompt = self.prompt_template.format(history=history_text, context=context, question=message)

            response = self.model.invoke(prompt)

            response_text = str(response.content)  # í…ìŠ¤íŠ¸ ë³€í™˜

            return response_text  # ğŸ”¹ history ì €ì¥ì€ API í•¨ìˆ˜ì—ì„œë§Œ!
        except Exception as e:
            print(f"Error during chat response generation: {e}")
            return "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def add_message_to_history(history, message, max_history=20):
    """
    Messageë¥¼ historyì— ì¶”ê°€í•˜ëŠ” ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ.
    """
    while len(history) >= max_history:
        history.pop(0)
    history.append(message)


# ë²¡í„° ìŠ¤í† ì–´ ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
COLLECTION_NAME = "bluer_db_openai"
PERSIST_DIRECTORY = "vector_store/chroma/bluer_db"
EMBEDDING_MODEL_NAME = 'text-embedding-3-small'

embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
vector_store = Chroma(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)

# Chatting í´ë˜ìŠ¤ ì´ˆê¸°í™”
chat = Chatting(vector_store=vector_store)
