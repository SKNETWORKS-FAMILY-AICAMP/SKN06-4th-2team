import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from cachetools import LRUCache
from time import sleep
from dotenv import load_dotenv
from textwrap import dedent
import traceback

load_dotenv()


class Chatting:
    """
    ëŒ€í™”í˜• AI ì±„íŒ… í´ë˜ìŠ¤.
    GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì™€ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•˜ê³ , ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    def __init__(self, vector_store, cache_size=100, rate_limit=0.1):
        self.rate_limit = rate_limit  # API í˜¸ì¶œ ì†ë„ ì œí•œ (ì´ˆ)
        self.cache = LRUCache(maxsize=cache_size)  # ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
        # print(vector_store._collection.count() ) # vectoryê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        # ëª¨ë¸ê³¼ ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        self.model = ChatOpenAI(model="gpt-4o-mini")
        self.retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 20,
                "fetch_k": 200,
                "lambda_mult": 0.7,
            }
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì •ì˜)
        self.prompt_template = dedent("""
        ë‹¹ì‹ ì€ í•œêµ­ì˜ ì‹ë‹¹ì„ ì¶”ì²œí•˜ê³  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë¹„ì„œì…ë‹ˆë‹¤.
        ë°˜ë“œì‹œ ì•„ë˜ [context]ì™€ [history]ì— ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”.
        [context]ë‚˜ [history]ì— ì—†ëŠ” ì •ë³´ì— ëŒ€í•´ì„œëŠ” "ë¬¸ë§¥ì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

        ì´ì „ ëŒ€í™”:
        {history}

        í˜„ì¬ ì°¸ê³  ê°€ëŠ¥í•œ ì •ë³´:
        {context}
                            
        [ëŒ€í™” ê·œì¹™]
        1. ì´ì „ ëŒ€í™”ì—ì„œ ê°€ê²Œ ì´ë¦„ì´ ì–¸ê¸‰ë˜ì§€ ì•Šê³  "ë‘ ê³³ ì¤‘", "ì´ ì¤‘ì—ì„œ", "ê°€ì¥ ì¢‹ì€ ê³³", "ë‘˜ ë‹¤" ë“±ì˜ í‘œí˜„ì´ ë“±ì¥í•  ê²½ìš°, [history]ì—ì„œ ê°€ì¥ ìµœê·¼ì— ë“±ì¥í•œ ë‘ ê°œì˜ ê°€ê²Œ ì´ë¦„ì„ ì°¾ì•„ì„œ [context]ì—ì„œ í•´ë‹¹ ê°€ê²Œ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.
        2. [context]ë‚˜ [history]ì— ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
        3.  **ë‹µë³€ì„ ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.**
            - ê°€ê²Œ ì´ë¦„ì€ `**êµµê²Œ ê°•ì¡°**`í•˜ì„¸ìš”.
            - ì„¸ë¶€ ì •ë³´ëŠ” `- í•­ëª©ë³„ ë¦¬ìŠ¤íŠ¸` í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
            - ì „í™”ë²ˆí˜¸, ê°€ê²©ëŒ€, ìœ„ì¹˜ ë“±ì˜ ì •ë³´ëŠ” `**ê°•ì¡°**`í•˜ì—¬ êµ¬ë¶„í•˜ì„¸ìš”.
        4. [context]ë‚˜ [history]ì— ì—†ëŠ” ì •ë³´ê°€ ìˆì„ ê²½ìš° ë°˜ë“œì‹œ "ë¬¸ë§¥ì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
        5. ì˜ëª»ëœ ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
        6. ë‹µë³€ì—ëŠ” ë¦¬ë³¸ê°œìˆ˜ë¥¼ í•­ìƒ í¬í•¨í•˜ë„ë¡ í•´ì£¼ì„¸ìš”.


        ì§ˆë¬¸: {question}
        ë‹µë³€:
        """)

    def get_cached_relevant_documents(self, query):
        """
        ê²€ìƒ‰ ìš”ì²­ì— ëŒ€í•´ ìºì‹± ë° API í˜¸ì¶œ ì†ë„ ì œí•œ ì ìš©.
        """
        if query in self.cache:
            return self.cache[query]
        try:
            sleep(self.rate_limit)  # API í˜¸ì¶œ ê°„ ë”œë ˆì´
            docs = self.retriever.invoke(query)
            context = "\n\n".join([doc.page_content for doc in docs])
            self.cache[query] = context
            return context
        except Exception as e:
            print(f"Error during document retrieval: {e}")
            traceback.print_exc()
            return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def send_message(self, message, history=None):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        try:
            context = self.get_cached_relevant_documents(message)

            history_text = ""
            for h in history:
                if h[0] == "human":
                    history_text += f"\nì‚¬ìš©ì: {h[1]}"
                else:
                    history_text += f"\nAI: {h[1]}"

            prompt = self.prompt_template.format(history=history_text, context=context, question=message)

            response = self.model.invoke(prompt)

            response_text = str(response.content)  # í…ìŠ¤íŠ¸ ë³€í™˜

            return response_text  # ğŸ”¹ history ì €ì¥ì€ API í•¨ìˆ˜ì—ì„œë§Œ!
        except Exception as e:
            print(f"Error during chat response generation: {e}")
            return "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def add_message_to_history(history, message, max_history=20):
    """
    Messageë¥¼ historyì— ì¶”ê°€í•˜ëŠ” ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ.
    """
    while len(history) >= max_history:
        history.pop(0)
    history.append(message)


# ë²¡í„° ìŠ¤í† ì–´ ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
COLLECTION_NAME = "bluer_db_openai"
PERSIST_DIRECTORY = "vector_store/chroma/bluer_db"
EMBEDDING_MODEL_NAME = 'text-embedding-3-small'

embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
vector_store = Chroma(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)

# Chatting í´ë˜ìŠ¤ ì´ˆê¸°í™”
chat = Chatting(vector_store=vector_store)
